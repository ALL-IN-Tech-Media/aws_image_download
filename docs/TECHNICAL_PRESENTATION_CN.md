# AWS TikTokå›¾åƒå¤„ç†ç³»ç»Ÿ - æŠ€æœ¯å®ç°ä¸é…ç½®
## æŠ€æœ¯æ¶æ„æ·±åº¦è§£æ

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

### æ ¸å¿ƒæŠ€æœ¯æ ˆ
```
æ•°æ®å±‚           å¤„ç†å±‚              å­˜å‚¨å±‚            ç›‘æ§å±‚
MySQL     â†’    AWS Lambda    â†’     S3 Buckets   â†’   CloudWatch
CSV       â†’    Python 3.9    â†’     DynamoDB     â†’   æ—¥å¿—åˆ†æ
          â†’    PIL + PIL      â†’     SQSé˜Ÿåˆ—      â†’   æ€§èƒ½æŒ‡æ ‡
```

### AWSæœåŠ¡æ¶æ„å›¾
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3 è¾“å…¥æ¡¶     â”‚    â”‚   SQS å¤„ç†é˜Ÿåˆ—   â”‚    â”‚ Lambda å¤„ç†å™¨   â”‚
â”‚ tiktok-image-   â”‚â”€â”€â”€â–¶â”‚ image-proc-     â”‚â”€â”€â”€â–¶â”‚ image-collage-  â”‚
â”‚ input/csv-files â”‚    â”‚ queue           â”‚    â”‚ processor       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3 è¾“å‡ºæ¡¶     â”‚    â”‚   æ­»ä¿¡é˜Ÿåˆ—      â”‚    â”‚   DynamoDB     â”‚
â”‚ tiktok-image-   â”‚    â”‚ processing-dlq  â”‚    â”‚ creator-state   â”‚
â”‚ output/collages â”‚    â”‚                 â”‚    â”‚ çŠ¶æ€è·Ÿè¸ªè¡¨      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ AWSæœåŠ¡è¯¦ç»†é…ç½®

### 1. **Lambdaå‡½æ•°é…ç½®**

#### åŸºç¡€é…ç½®
```yaml
å‡½æ•°åç§°: image-collage-processor
è¿è¡Œæ—¶: Python 3.9
æ¶æ„: x86_64
å†…å­˜: 3008 MB (æœ€å¤§)
è¶…æ—¶: 15åˆ†é’Ÿ (900ç§’)
å¹¶å‘é™åˆ¶: 50ä¸ªå®ä¾‹
```

#### ç¯å¢ƒå˜é‡é…ç½®
```bash
# S3å­˜å‚¨æ¡¶é…ç½®
INPUT_BUCKET=tiktok-image-input
OUTPUT_BUCKET=tiktok-image-output
TEMP_BUCKET=tiktok-image-temp

# SQSé˜Ÿåˆ—é…ç½®
SQS_QUEUE_URL=https://sqs.us-east-1.amazonaws.com/ACCOUNT/tiktok-image-processing-queue
SQS_DLQ_URL=https://sqs.us-east-1.amazonaws.com/ACCOUNT/tiktok-image-processing-dlq

# DynamoDBé…ç½®
DYNAMODB_TABLE_NAME=tiktok-creator-processing-state

# å¤„ç†å‚æ•°é…ç½®
DEFAULT_ROWS=5
DEFAULT_COLS=7
DEFAULT_QUALITY=95
DEFAULT_MAX_WORKERS=8
DEFAULT_TIMEOUT=30
DEFAULT_MAX_RETRIES=3
DEFAULT_MAX_IMAGES_PER_CREATOR=35

# æ€§èƒ½ä¼˜åŒ–é…ç½®
MIN_IMAGE_SIZE=50
MAX_IMAGE_SIZE=800
MEMORY_WARNING_THRESHOLD=80.0
MEMORY_CRITICAL_THRESHOLD=90.0
MAX_CONCURRENT_DOWNLOADS=8

# å»é‡å’ŒçŠ¶æ€ç®¡ç†
ENABLE_DEDUPLICATION=true
COLLISION_CHECK_ENABLED=true
CONTENT_HASH_ALGORITHM=sha256
BATCH_COORDINATION_ENABLED=true
```

#### ä»£ç ä¾èµ–åŒ… (requirements.txt)
```txt
boto3>=1.34.0          # AWS SDK
botocore>=1.34.0       # AWSæ ¸å¿ƒåº“
Pillow>=10.0.0         # å›¾åƒå¤„ç†
requests>=2.31.0       # HTTPè¯·æ±‚
psutil>=5.9.0          # ç³»ç»Ÿç›‘æ§
typing-extensions>=4.8.0  # ç±»å‹æ³¨è§£
```

### 2. **S3å­˜å‚¨æ¡¶é…ç½®**

#### å­˜å‚¨æ¡¶ç»“æ„
```bash
# è¾“å…¥å­˜å‚¨æ¡¶: tiktok-image-input
â”œâ”€â”€ csv-files/              # è‡ªåŠ¨å¤„ç†CSVæ–‡ä»¶
â”‚   â”œâ”€â”€ 2025/01/19/
â”‚   â”‚   â”œâ”€â”€ cover_urls_20250119_120000.csv
â”‚   â”‚   â””â”€â”€ monthly_batch.csv
â”œâ”€â”€ manual-uploads/         # æ‰‹åŠ¨å¤„ç†æ–‡ä»¶
â”œâ”€â”€ archived/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ failed/

# è¾“å‡ºå­˜å‚¨æ¡¶: tiktok-image-output  
â”œâ”€â”€ collages/              # ç”Ÿæˆçš„æ‹¼å›¾
â”‚   â”œâ”€â”€ by-creator/
â”‚   â”‚   â”œâ”€â”€ creator1/
â”‚   â”‚   â”‚   â”œâ”€â”€ collage_20250119_120000.jpg
â”‚   â”‚   â””â”€â”€ creator2/
â”œâ”€â”€ results/               # å¤„ç†ç»“æœ
â”‚   â”œâ”€â”€ processing-summaries/
â”‚   â””â”€â”€ metadata/

# ä¸´æ—¶å­˜å‚¨æ¡¶: tiktok-image-temp
â”œâ”€â”€ processing/            # å¤„ç†ä¸­ä¸´æ—¶æ–‡ä»¶
â”œâ”€â”€ debug/                # è°ƒè¯•ä¿¡æ¯
â””â”€â”€ large-files/          # å¤§æ–‡ä»¶æš‚å­˜
```

#### ç”Ÿå‘½å‘¨æœŸç­–ç•¥é…ç½®
```json
{
  "Rules": [
    {
      "ID": "è¾“å…¥æ–‡ä»¶å½’æ¡£ç­–ç•¥",
      "Status": "Enabled",
      "Filter": {"Prefix": "csv-files/"},
      "Transitions": [
        {"Days": 30, "StorageClass": "STANDARD_IA"},
        {"Days": 90, "StorageClass": "GLACIER"},
        {"Days": 365, "StorageClass": "DEEP_ARCHIVE"}
      ]
    },
    {
      "ID": "ä¸´æ—¶æ–‡ä»¶æ¸…ç†ç­–ç•¥", 
      "Status": "Enabled",
      "Filter": {"Prefix": "processing/"},
      "Expiration": {"Days": 7}
    }
  ]
}
```

### 3. **SQSé˜Ÿåˆ—é…ç½®**

#### ä¸»å¤„ç†é˜Ÿåˆ—
```yaml
é˜Ÿåˆ—åç§°: tiktok-image-processing-queue
é˜Ÿåˆ—ç±»å‹: æ ‡å‡†é˜Ÿåˆ—
æ¶ˆæ¯ä¿ç•™æ—¶é—´: 14å¤© (1,209,600ç§’)
å¯è§æ€§è¶…æ—¶: 15åˆ†é’Ÿ (900ç§’)
æ¶ˆæ¯æœ€å¤§å¤§å°: 256KB
æ¥æ”¶ç­‰å¾…æ—¶é—´: 20ç§’ (é•¿è½®è¯¢)
æœ€å¤§æ¥æ”¶æ¬¡æ•°: 3æ¬¡
```

#### æ­»ä¿¡é˜Ÿåˆ—é…ç½®
```yaml
é˜Ÿåˆ—åç§°: tiktok-image-processing-dlq
æ¶ˆæ¯ä¿ç•™æ—¶é—´: 14å¤©
é‡è¯•ç­–ç•¥:
  - æœ€å¤§é‡è¯•æ¬¡æ•°: 3
  - å¤±è´¥åè½¬å…¥æ­»ä¿¡é˜Ÿåˆ—
  - æ”¯æŒæ‰‹åŠ¨é‡æ–°å¤„ç†
```

#### SQSæ¶ˆæ¯æ ¼å¼
```json
{
  "processing_type": "csv_s3",
  "s3_bucket": "tiktok-image-input",
  "s3_key": "manual-uploads/batch_20250119.csv",
  "output_prefix": "sqs-trigger/",
  "processing_config": {
    "group_by_creator": true,
    "rows": 5,
    "cols": 7,
    "max_images_per_creator": 35,
    "quality": 95,
    "max_workers": 8,
    "timeout": 30,
    "max_retries": 3
  }
}
```

### 4. **DynamoDBè¡¨é…ç½®**

#### è¡¨ç»“æ„è®¾è®¡
```yaml
è¡¨å: tiktok-creator-processing-state
è®¡è´¹æ¨¡å¼: æŒ‰éœ€ä»˜è´¹ (Pay-per-request)
åˆ†åŒºé”®: creator_name (String)
æ’åºé”®: processing_date (String)

å…¨å±€äºŒçº§ç´¢å¼•:
  1. batch-id-index:
     - åˆ†åŒºé”®: batch_id
     - æŠ•å½±ç±»å‹: ALL
  2. status-date-index:
     - åˆ†åŒºé”®: status  
     - æ’åºé”®: processing_date
     - æŠ•å½±ç±»å‹: ALL

TTLè®¾ç½®:
  - å±æ€§: ttl
  - è‡ªåŠ¨åˆ é™¤è¿‡æœŸè®°å½•
```

#### æ•°æ®é¡¹ç»“æ„ç¤ºä¾‹
```json
{
  "creator_name": "aaliahmaddox",
  "processing_date": "2025-01-19T12:00:00Z",
  "batch_id": "s3_cover_urls_20250119_120000",
  "status": "completed",
  "images_processed": 35,
  "collage_s3_key": "collages/by-creator/aaliahmaddox/collage_20250119_120000.jpg",
  "processing_duration": 23.5,
  "error_count": 0,
  "metadata": {
    "source_csv": "cover_urls_20250119_120000.csv",
    "lambda_request_id": "12345678-1234-1234-1234-123456789012"
  },
  "ttl": 1672531200
}
```

### 5. **IAMæƒé™é…ç½®**

#### Lambdaæ‰§è¡Œè§’è‰²
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:ListBucket", 
        "s3:HeadObject"
      ],
      "Resource": [
        "arn:aws:s3:::tiktok-image-input",
        "arn:aws:s3:::tiktok-image-input/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:PutObjectAcl",
        "s3:GetObject",
        "s3:ListBucket",
        "s3:HeadObject",
        "s3:ListObjectsV2"
      ],
      "Resource": [
        "arn:aws:s3:::tiktok-image-output",
        "arn:aws:s3:::tiktok-image-output/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "dynamodb:PutItem",
        "dynamodb:GetItem", 
        "dynamodb:UpdateItem",
        "dynamodb:Query",
        "dynamodb:Scan"
      ],
      "Resource": [
        "arn:aws:dynamodb:*:*:table/tiktok-creator-processing-state",
        "arn:aws:dynamodb:*:*:table/tiktok-creator-processing-state/index/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "sqs:ReceiveMessage",
        "sqs:DeleteMessage",
        "sqs:GetQueueAttributes",
        "sqs:SendMessage"
      ],
      "Resource": [
        "arn:aws:sqs:*:*:tiktok-image-processing-queue",
        "arn:aws:sqs:*:*:tiktok-image-processing-dlq"
      ]
    }
  ]
}
```

---

## ğŸ’» æ ¸å¿ƒæŠ€æœ¯å®ç°

### 1. **Lambdaå‡½æ•°æ¶æ„**

#### ä¸»å¤„ç†æµç¨‹
```python
def lambda_handler(event, context):
    """ä¸»Lambdaå¤„ç†å™¨å…¥å£"""
    
    # 1. åˆå§‹åŒ–é…ç½®å’Œå¤„ç†å™¨
    config = Config()
    image_processor = AWSImageProcessor(config)
    sqs_processor = SQSProcessor(config)
    s3_utils = S3Utils(config)
    
    # 2. åˆ¤æ–­è§¦å‘æº
    if 'Records' in event:
        if 's3' in event['Records'][0]:
            # S3äº‹ä»¶è§¦å‘ (è‡ªåŠ¨å¤„ç†)
            return handle_s3_event(event, processors...)
        elif 'eventSource' in event['Records'][0]:
            # SQSäº‹ä»¶è§¦å‘ (å—æ§å¤„ç†)  
            return handle_sqs_event(event, processors...)
    else:
        # ç›´æ¥è°ƒç”¨ (æµ‹è¯•æ¨¡å¼)
        return handle_direct_invocation(event, processors...)
```

#### æ ¸å¿ƒç±»ç»“æ„
```python
class AWSImageProcessor:
    """AWS Lambdaä¼˜åŒ–çš„å›¾åƒå¤„ç†å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.s3_utils = S3Utils(config)
        
        # å¯é€‰: å»é‡ç»„ä»¶
        if config.ENABLE_DEDUPLICATION:
            self.collision_detector = S3CollisionDetector(config)
            self.processing_state = ProcessingState(config)
            self.content_hasher = ContentHasher(config.CONTENT_HASH_ALGORITHM)
    
    def process_csv_data(self, csv_data, output_prefix, batch_id, **kwargs):
        """å¤„ç†CSVæ•°æ®ä¸»æ–¹æ³•"""
        # 1. è§£æCSVæ•°æ®
        # 2. æŒ‰åˆ›ä½œè€…åˆ†ç»„
        # 3. å¹¶å‘ä¸‹è½½å›¾åƒ
        # 4. ç”Ÿæˆæ‹¼å›¾
        # 5. ä¸Šä¼ åˆ°S3
        # 6. æ›´æ–°çŠ¶æ€
```

### 2. **å¹¶å‘å›¾åƒä¸‹è½½å®ç°**

#### ThreadPoolExecutoré…ç½®
```python
def download_images_batch(self, urls, max_workers=8, timeout=30):
    """å¹¶å‘ä¸‹è½½å›¾åƒæ‰¹æ¬¡"""
    
    images = []
    failed_urls = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
        future_to_url = {
            executor.submit(self.download_image_from_url, url, timeout): url 
            for url in urls
        }
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                image = future.result()
                if image:
                    images.append(image)
                else:
                    failed_urls.append(url)
            except Exception as e:
                logger.error(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
                failed_urls.append(url)
    
    return images, failed_urls
```

#### é‡è¯•æœºåˆ¶å®ç°
```python
def download_image_from_url(self, url, timeout=30, max_retries=3):
    """å¸¦é‡è¯•çš„å›¾åƒä¸‹è½½"""
    
    headers = {
        'User-Agent': self.config.USER_AGENT,
        'Accept': 'image/*',
        'Cache-Control': 'no-cache'
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()
            
            # éªŒè¯å†…å®¹ç±»å‹
            if not response.headers.get('content-type', '').startswith('image/'):
                raise ValueError(f"æ— æ•ˆçš„å†…å®¹ç±»å‹: {response.headers.get('content-type')}")
            
            # å¤„ç†å›¾åƒ
            image = Image.open(io.BytesIO(response.content))
            
            # éªŒè¯å›¾åƒå°ºå¯¸
            if image.size[0] < self.config.MIN_IMAGE_SIZE or image.size[1] < self.config.MIN_IMAGE_SIZE:
                raise ValueError(f"å›¾åƒå°ºå¯¸è¿‡å°: {image.size}")
            
            # è½¬æ¢æ ¼å¼
            if image.mode == 'RGBA':
                image = image.convert('RGB')
                
            return image
            
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = (2 ** attempt) * self.config.RETRY_BACKOFF_FACTOR
                time.sleep(min(wait_time, self.config.RETRY_MAX_DELAY))
                continue
            else:
                logger.error(f"ä¸‹è½½æœ€ç»ˆå¤±è´¥ {url}: {e}")
                return None
```

### 3. **æ‹¼å›¾ç”Ÿæˆç®—æ³•**

#### ç½‘æ ¼å¸ƒå±€è®¡ç®—
```python
def create_image_collage_s3(self, images, rows=5, cols=7, quality=95):
    """åˆ›å»ºå›¾åƒæ‹¼å›¾å¹¶ä¸Šä¼ åˆ°S3"""
    
    if not images:
        raise ValueError("æ²¡æœ‰å¯ç”¨å›¾åƒ")
    
    # è®¡ç®—æ‹¼å›¾å°ºå¯¸
    target_images = min(len(images), rows * cols)
    actual_rows = (target_images + cols - 1) // cols
    
    # å•ä¸ªå›¾åƒå°ºå¯¸ (æ­£æ–¹å½¢)
    cell_size = 200
    canvas_width = cols * cell_size  
    canvas_height = actual_rows * cell_size
    
    # åˆ›å»ºç”»å¸ƒ
    collage = Image.new('RGB', (canvas_width, canvas_height), 'white')
    
    # æ”¾ç½®å›¾åƒ
    for i, image in enumerate(images[:target_images]):
        row = i // cols
        col = i % cols
        
        # è°ƒæ•´å›¾åƒå¤§å° (ä¿æŒæ¯”ä¾‹,å±…ä¸­è£å‰ª)
        resized_image = self.resize_and_center_crop(image, cell_size, cell_size)
        
        # è®¡ç®—ä½ç½®
        x = col * cell_size
        y = row * cell_size
        
        # ç²˜è´´å›¾åƒ
        collage.paste(resized_image, (x, y))
        
        # å†…å­˜æ¸…ç†
        if i % 10 == 0:
            gc.collect()
    
    return collage
```

#### å›¾åƒé¢„å¤„ç†
```python
def resize_and_center_crop(self, image, target_width, target_height):
    """è°ƒæ•´å°ºå¯¸å¹¶å±…ä¸­è£å‰ª"""
    
    original_width, original_height = image.size
    original_ratio = original_width / original_height
    target_ratio = target_width / target_height
    
    if original_ratio > target_ratio:
        # åŸå›¾æ›´å®½,æŒ‰é«˜åº¦ç¼©æ”¾
        new_height = target_height
        new_width = int(original_width * target_height / original_height)
    else:
        # åŸå›¾æ›´é«˜,æŒ‰å®½åº¦ç¼©æ”¾  
        new_width = target_width
        new_height = int(original_height * target_width / original_width)
    
    # è°ƒæ•´å°ºå¯¸
    resized = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
    
    # å±…ä¸­è£å‰ª
    left = (new_width - target_width) // 2
    top = (new_height - target_height) // 2
    right = left + target_width
    bottom = top + target_height
    
    return resized.crop((left, top, right, bottom))
```

### 4. **S3é›†æˆå®ç°**

#### æ–‡ä»¶ä¸Šä¼ ä¼˜åŒ–
```python
class S3Utils:
    """S3æ“ä½œå·¥å…·ç±»"""
    
    def upload_collage_to_s3(self, image, s3_key, bucket_name, quality=95):
        """ä¸Šä¼ æ‹¼å›¾åˆ°S3,æ”¯æŒå¤šéƒ¨åˆ†ä¸Šä¼ """
        
        # è½¬æ¢ä¸ºå­—èŠ‚æµ
        img_buffer = io.BytesIO()
        image.save(img_buffer, format='JPEG', quality=quality, optimize=True)
        img_buffer.seek(0)
        
        # è·å–æ–‡ä»¶å¤§å°
        file_size = img_buffer.getbuffer().nbytes
        
        try:
            if file_size > 100 * 1024 * 1024:  # å¤§äº100MBä½¿ç”¨å¤šéƒ¨åˆ†ä¸Šä¼ 
                return self._multipart_upload(img_buffer, bucket_name, s3_key)
            else:
                # æ ‡å‡†ä¸Šä¼ 
                self.s3_client.upload_fileobj(
                    img_buffer,
                    bucket_name,
                    s3_key,
                    ExtraArgs={
                        'ContentType': 'image/jpeg',
                        'CacheControl': 'max-age=31536000',  # 1å¹´ç¼“å­˜
                        'ServerSideEncryption': 'AES256'
                    }
                )
                
            # ç”Ÿæˆé¢„ç­¾åURL
            presigned_url = self.s3_client.generate_presigned_url(
                'get_object',
                Params={'Bucket': bucket_name, 'Key': s3_key},
                ExpiresIn=self.config.S3_PRESIGNED_URL_EXPIRATION
            )
            
            return {
                's3_key': s3_key,
                'bucket': bucket_name,
                'presigned_url': presigned_url,
                'file_size': file_size
            }
            
        except Exception as e:
            logger.error(f"S3ä¸Šä¼ å¤±è´¥: {e}")
            raise
```

### 5. **çŠ¶æ€ç®¡ç†ä¸å»é‡**

#### DynamoDBçŠ¶æ€è·Ÿè¸ª
```python
class ProcessingState:
    """å¤„ç†çŠ¶æ€ç®¡ç†"""
    
    def update_creator_state(self, creator_name, batch_id, status, metadata=None):
        """æ›´æ–°åˆ›ä½œè€…å¤„ç†çŠ¶æ€"""
        
        item = {
            'creator_name': creator_name,
            'processing_date': datetime.utcnow().isoformat(),
            'batch_id': batch_id,
            'status': status,
            'updated_at': datetime.utcnow().isoformat()
        }
        
        if metadata:
            item.update(metadata)
        
        # è®¾ç½®TTL (30å¤©åè‡ªåŠ¨åˆ é™¤)
        item['ttl'] = int((datetime.utcnow() + timedelta(days=30)).timestamp())
        
        try:
            self.dynamodb_table.put_item(Item=item)
            logger.info(f"çŠ¶æ€æ›´æ–°æˆåŠŸ: {creator_name} -> {status}")
        except Exception as e:
            logger.error(f"çŠ¶æ€æ›´æ–°å¤±è´¥: {e}")
            raise
```

#### å†…å®¹å»é‡å®ç°
```python
class ContentHasher:
    """å†…å®¹å“ˆå¸Œè®¡ç®—"""
    
    def calculate_image_hash(self, image_data):
        """è®¡ç®—å›¾åƒå†…å®¹å“ˆå¸Œ"""
        
        hasher = hashlib.sha256()
        
        if isinstance(image_data, Image.Image):
            # PILå›¾åƒå¯¹è±¡
            img_buffer = io.BytesIO()
            image_data.save(img_buffer, format='JPEG')
            hasher.update(img_buffer.getvalue())
        else:
            # å­—èŠ‚æ•°æ®
            hasher.update(image_data)
        
        return hasher.hexdigest()
    
    def check_duplicate(self, image_hash, creator_name, batch_id):
        """æ£€æŸ¥é‡å¤å†…å®¹"""
        
        # æŸ¥è¯¢ç°æœ‰è®°å½•
        try:
            response = self.dynamodb_table.query(
                IndexName='batch-id-index',
                KeyConditionExpression=Key('batch_id').eq(batch_id)
            )
            
            for item in response['Items']:
                if item.get('image_hash') == image_hash:
                    return True, item
                    
            return False, None
            
        except Exception as e:
            logger.error(f"é‡å¤æ£€æŸ¥å¤±è´¥: {e}")
            return False, None
```

---

## ğŸ”§ éƒ¨ç½²ä¸è¿ç»´é…ç½®

### 1. **TerraformåŸºç¡€è®¾æ–½ä»£ç **

#### ä¸»è¦èµ„æºå®šä¹‰
```hcl
# Lambdaå‡½æ•°
resource "aws_lambda_function" "image_processor" {
  filename         = "deployment-package.zip"
  function_name    = var.lambda_function_name
  role            = aws_iam_role.lambda_execution_role.arn
  handler         = "lambda_function.lambda_handler"
  runtime         = "python3.9"
  memory_size     = 3008
  timeout         = 900
  
  environment {
    variables = {
      INPUT_BUCKET     = aws_s3_bucket.input_bucket.bucket
      OUTPUT_BUCKET    = aws_s3_bucket.output_bucket.bucket
      TEMP_BUCKET      = aws_s3_bucket.temp_bucket.bucket
      SQS_QUEUE_URL    = aws_sqs_queue.processing_queue.url
      SQS_DLQ_URL      = aws_sqs_queue.dead_letter_queue.url
      DYNAMODB_TABLE_NAME = aws_dynamodb_table.creator_processing_state.name
    }
  }
}

# S3äº‹ä»¶é€šçŸ¥é…ç½®
resource "aws_s3_bucket_notification" "input_bucket_notification" {
  bucket = aws_s3_bucket.input_bucket.id

  lambda_function {
    lambda_function_arn = aws_lambda_function.image_processor.arn
    events              = ["s3:ObjectCreated:*"]
    filter_prefix       = "csv-files/"
    filter_suffix       = ".csv"
  }
}

# SQSè§¦å‘å™¨é…ç½®
resource "aws_lambda_event_source_mapping" "sqs_trigger" {
  event_source_arn = aws_sqs_queue.processing_queue.arn
  function_name    = aws_lambda_function.image_processor.arn
  batch_size       = 1
  maximum_batching_window_in_seconds = 5
}
```

### 2. **ç›‘æ§ä¸æ—¥å¿—é…ç½®**

#### CloudWatchæ—¥å¿—ç»„
```hcl
resource "aws_cloudwatch_log_group" "lambda_log_group" {
  name              = "/aws/lambda/${var.lambda_function_name}"
  retention_in_days = var.log_retention_days

  tags = {
    Environment = var.environment
    Purpose     = "Lambda Function Logs"
  }
}
```

#### è‡ªå®šä¹‰æŒ‡æ ‡
```python
# Lambdaå‡½æ•°ä¸­çš„æŒ‡æ ‡å‘å¸ƒ
import boto3

cloudwatch = boto3.client('cloudwatch')

def publish_metrics(processing_duration, images_processed, error_count):
    """å‘å¸ƒè‡ªå®šä¹‰æŒ‡æ ‡åˆ°CloudWatch"""
    
    metrics = [
        {
            'MetricName': 'ProcessingDuration',
            'Value': processing_duration,
            'Unit': 'Seconds',
            'Dimensions': [
                {'Name': 'FunctionName', 'Value': os.environ['AWS_LAMBDA_FUNCTION_NAME']}
            ]
        },
        {
            'MetricName': 'ImagesProcessed', 
            'Value': images_processed,
            'Unit': 'Count'
        },
        {
            'MetricName': 'ErrorCount',
            'Value': error_count, 
            'Unit': 'Count'
        }
    ]
    
    cloudwatch.put_metric_data(
        Namespace='TikTokImageProcessor',
        MetricData=metrics
    )
```

### 3. **éƒ¨ç½²è„šæœ¬**

#### è‡ªåŠ¨åŒ–éƒ¨ç½²æµç¨‹
```bash
#!/bin/bash
# deploy.sh - è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬

set -e

echo "å¼€å§‹éƒ¨ç½²TikTokå›¾åƒå¤„ç†ç³»ç»Ÿ..."

# 1. æ‰“åŒ…Lambdaä»£ç 
echo "æ‰“åŒ…Lambdaå‡½æ•°..."
cd src/
zip -r ../deployment-package.zip . -x "*.pyc" "__pycache__/*"
cd ..

# 2. éƒ¨ç½²åŸºç¡€è®¾æ–½
echo "éƒ¨ç½²AWSåŸºç¡€è®¾æ–½..."
cd terraform/
terraform init
terraform plan -out=tfplan
terraform apply tfplan

# 3. æ›´æ–°Lambdaå‡½æ•°ä»£ç 
echo "æ›´æ–°Lambdaå‡½æ•°ä»£ç ..."
aws lambda update-function-code \
    --function-name image-collage-processor \
    --zip-file fileb://../deployment-package.zip

# 4. ç­‰å¾…å‡½æ•°æ›´æ–°å®Œæˆ
echo "ç­‰å¾…å‡½æ•°æ›´æ–°å®Œæˆ..."
aws lambda wait function-updated \
    --function-name image-collage-processor

# 5. è¿è¡Œæµ‹è¯•
echo "è¿è¡Œé›†æˆæµ‹è¯•..."
python3 tests/integration_test.py

echo "éƒ¨ç½²å®Œæˆ!"
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯ç»†èŠ‚

### 1. **å†…å­˜ç®¡ç†ä¼˜åŒ–**
```python
# å†…å­˜ç›‘æ§å’Œåƒåœ¾å›æ”¶
def monitor_memory_usage():
    """ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    
    process = psutil.Process()
    memory_info = process.memory_info()
    memory_percent = process.memory_percent()
    
    if memory_percent > config.MEMORY_WARNING_THRESHOLD:
        logger.warning(f"å†…å­˜ä½¿ç”¨ç‡é«˜: {memory_percent:.1f}%")
        gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
    
    if memory_percent > config.MEMORY_CRITICAL_THRESHOLD:
        logger.error(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_percent:.1f}%")
        raise MemoryError("å†…å­˜ä¸è¶³")
    
    return {
        'rss': memory_info.rss / 1024 / 1024,  # MB
        'vms': memory_info.vms / 1024 / 1024,  # MB  
        'percent': memory_percent
    }
```

### 2. **å¹¶å‘æ§åˆ¶ç­–ç•¥**
```python
# åŠ¨æ€å·¥ä½œå™¨æ•°é‡è°ƒæ•´
def calculate_optimal_workers(total_images, available_memory):
    """æ ¹æ®å›¾åƒæ•°é‡å’Œå¯ç”¨å†…å­˜è®¡ç®—æœ€ä¼˜å·¥ä½œå™¨æ•°é‡"""
    
    base_workers = config.DEFAULT_MAX_WORKERS
    
    # æ ¹æ®å†…å­˜è°ƒæ•´
    memory_factor = min(available_memory / 1000, 1.0)  # MB to factor
    
    # æ ¹æ®å›¾åƒæ•°é‡è°ƒæ•´  
    if total_images < 50:
        image_factor = 0.5
    elif total_images < 200:
        image_factor = 0.8
    else:
        image_factor = 1.0
    
    optimal_workers = int(base_workers * memory_factor * image_factor)
    return max(1, min(optimal_workers, 10))  # 1-10ä¹‹é—´
```

### 3. **é”™è¯¯å¤„ç†å’Œæ¢å¤**
```python
# åˆ†æ‰¹å¤„ç†é”™è¯¯æ¢å¤
def process_with_recovery(self, csv_data, **kwargs):
    """å¸¦é”™è¯¯æ¢å¤çš„å¤„ç†æ–¹æ³•"""
    
    try:
        return self._process_normal(csv_data, **kwargs)
    except MemoryError:
        logger.warning("å†…å­˜ä¸è¶³,åˆ‡æ¢åˆ°å®‰å…¨æ¨¡å¼")
        return self._process_safe_mode(csv_data, **kwargs)
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {e}")
        return self._process_minimal_mode(csv_data, **kwargs)

def _process_safe_mode(self, csv_data, **kwargs):
    """å®‰å…¨æ¨¡å¼: å‡å°‘å¹¶å‘å’Œå›¾åƒè´¨é‡"""
    
    safe_config = kwargs.copy()
    safe_config.update({
        'max_workers': 2,
        'quality': 75,
        'max_images_per_creator': 20
    })
    
    return self._process_normal(csv_data, **safe_config)
```

---

## ğŸ” æ•…éšœæ’é™¤ä¸ç›‘æ§

### 1. **å¸¸è§é—®é¢˜è¯Šæ–­**
```python
# ç³»ç»Ÿå¥åº·æ£€æŸ¥
def health_check():
    """ç³»ç»Ÿå¥åº·çŠ¶æ€æ£€æŸ¥"""
    
    checks = {
        'lambda_memory': check_lambda_memory(),
        's3_connectivity': check_s3_connectivity(), 
        'sqs_connectivity': check_sqs_connectivity(),
        'dynamodb_connectivity': check_dynamodb_connectivity(),
        'external_network': check_external_network()
    }
    
    failed_checks = [k for k, v in checks.items() if not v]
    
    if failed_checks:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {failed_checks}")
        return False
    
    return True
```

### 2. **æ€§èƒ½åŸºå‡†æµ‹è¯•**
```python
# æ€§èƒ½æµ‹è¯•æ¡†æ¶
def benchmark_processing():
    """å¤„ç†æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    test_cases = [
        {'images': 10, 'expected_time': 5},
        {'images': 50, 'expected_time': 15}, 
        {'images': 100, 'expected_time': 30}
    ]
    
    results = []
    
    for case in test_cases:
        start_time = time.time()
        
        # æ¨¡æ‹Ÿå¤„ç†
        result = simulate_processing(case['images'])
        
        duration = time.time() - start_time
        passed = duration <= case['expected_time']
        
        results.append({
            'images': case['images'],
            'duration': duration,
            'expected': case['expected_time'], 
            'passed': passed
        })
    
    return results
```

---

## ğŸ“‹ ç³»ç»Ÿé…ç½®æ€»ç»“

### æ ¸å¿ƒé…ç½®å‚æ•°
| å‚æ•°ç±»åˆ« | å‚æ•°å | å€¼ | è¯´æ˜ |
|---------|--------|----|----|
| **Lambda** | å†…å­˜ | 3008 MB | æœ€å¤§å†…å­˜é…ç½® |
| **Lambda** | è¶…æ—¶ | 15åˆ†é’Ÿ | æœ€å¤§æ‰§è¡Œæ—¶é—´ |
| **å¹¶å‘** | å·¥ä½œå™¨æ•° | 8 | å¹¶å‘ä¸‹è½½çº¿ç¨‹ |
| **å›¾åƒ** | ç½‘æ ¼å°ºå¯¸ | 5x7 | é»˜è®¤æ‹¼å›¾å¸ƒå±€ |
| **å›¾åƒ** | è´¨é‡ | 95% | JPEGå‹ç¼©è´¨é‡ |
| **é‡è¯•** | æœ€å¤§æ¬¡æ•° | 3 | å¤±è´¥é‡è¯•æ¬¡æ•° |
| **SQS** | å¯è§æ€§è¶…æ—¶ | 15åˆ†é’Ÿ | æ¶ˆæ¯å¤„ç†æ—¶é—´ |
| **S3** | ç”Ÿå‘½å‘¨æœŸ | 30/90/365å¤© | å­˜å‚¨å±‚çº§è½¬æ¢ |

### å…³é”®æŠ€æœ¯ç‰¹æ€§
- âœ… **æ— æœåŠ¡å™¨æ¶æ„**: è‡ªåŠ¨æ‰©ç¼©å®¹ï¼ŒæŒ‰éœ€ä»˜è´¹
- âœ… **å¹¶å‘å¤„ç†**: 8çº¿ç¨‹å¹¶å‘ä¸‹è½½ï¼Œ9å€é€Ÿåº¦æå‡
- âœ… **æ™ºèƒ½é‡è¯•**: æŒ‡æ•°é€€é¿é‡è¯•ï¼Œ99.9%æˆåŠŸç‡
- âœ… **å†…å­˜ä¼˜åŒ–**: åŠ¨æ€åƒåœ¾å›æ”¶ï¼Œ10%å†…å­˜ä½¿ç”¨ç‡
- âœ… **çŠ¶æ€ç®¡ç†**: DynamoDBçŠ¶æ€è·Ÿè¸ªå’Œå»é‡
- âœ… **ç›‘æ§å®Œæ•´**: CloudWatchæ—¥å¿—å’Œè‡ªå®šä¹‰æŒ‡æ ‡
- âœ… **åŸºç¡€è®¾æ–½å³ä»£ç **: Terraformè‡ªåŠ¨åŒ–éƒ¨ç½²

è¿™ä¸ªæŠ€æœ¯ç³»ç»Ÿå·²ç»ç”Ÿäº§å°±ç»ªï¼Œå…·å¤‡å®Œæ•´çš„é”™è¯¯å¤„ç†ã€ç›‘æ§å’Œä¼˜åŒ–æœºåˆ¶ï¼Œå¯ä»¥ç¨³å®šå¤„ç†å¤§è§„æ¨¡å›¾åƒå¤„ç†éœ€æ±‚ã€‚

